{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e10334-3ead-44a1-a920-d6ca7fa66a5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Universal Sentence Encoderを使って文章の異常検知をする\n",
    "詳細な説明は https://qiita.com/jovyan/items/e5d2dc7ffabc2353db38 をご覧ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ff75a-180d-434d-8ded-fbdefd7f975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0faed6e-c14d-425f-938e-6371742d26c5",
   "metadata": {},
   "source": [
    "## テキストデータ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e3903-f476-4a3a-a26f-637ba76e1f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 青空文庫から夏目漱石の小説をダウンロード\n",
    "url_list = [\n",
    "    'https://www.aozora.gr.jp/cards/000148/files/773_14560.html',  # 彼岸過迄\n",
    "    'https://www.aozora.gr.jp/cards/000148/files/765_14961.html',  # 行人\n",
    "    'https://www.aozora.gr.jp/cards/000148/files/775_14942.html',  # こころ\n",
    "]\n",
    "\n",
    "def fetch_sentences(url):\n",
    "    \"\"\"\n",
    "    urlからテキストをスクレイピングしてルビを除く\n",
    "    \"\"\"\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser', from_encoding='utf-8')\n",
    "    main_text = soup.find(\"div\", class_='main_text')\n",
    "    #ルビが振ってあるのを削除\n",
    "    for yomigana in main_text.find_all([\"rp\",\"h4\",\"rt\"]):\n",
    "        yomigana.decompose()\n",
    "    sentences = [line.strip() for line in main_text.text.strip().splitlines()]\n",
    "    return sentences\n",
    "\n",
    "souseki_texts = []\n",
    "for url in url_list:\n",
    "    souseki_texts += fetch_sentences(url)\n",
    "\n",
    "print(len(souseki_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480978b-2c4e-41b6-97fe-0b57eabf4335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_texts(texts_input):\n",
    "    \"\"\"\n",
    "    文章を句点ごとに分割する\n",
    "    \"\"\"\n",
    "    texts_output = []\n",
    "    for t in texts_input:\n",
    "        texts_output += t.split('。')\n",
    "    return texts_output\n",
    "\n",
    "souseki_texts = split_texts(souseki_texts)\n",
    "print(len(souseki_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3632d33-68ce-4cfc-870e-06e52e9f10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_length(texts_input, min_len=20, max_len=300):\n",
    "    \"\"\"\n",
    "    短すぎる文章と長すぎる文章を除く\n",
    "    \"\"\"\n",
    "    texts_output = []\n",
    "    for s in texts_input:\n",
    "        length = len(s)\n",
    "        if length >= min_len and length <= max_len:\n",
    "            texts_output.append(s)\n",
    "    return texts_output\n",
    "\n",
    "souseki_texts = filter_by_length(souseki_texts)\n",
    "print(len(souseki_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff6e7f-8368-4d60-992d-b069fac500de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chABSA-dataset（有価証券報告書）をダウンロード\n",
    "!wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/chABSA-dataset.zip && \\\n",
    " unzip chABSA-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260804bd-6825-4034-9442-aaaadaf6f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonファイルを展開して文章をリストに格納\n",
    "data_path = Path(\"chABSA-dataset\")\n",
    "path_list = [p for p in data_path.glob(\"*.json\")]\n",
    "\n",
    "chabsa_texts = []\n",
    "for p in path_list:\n",
    "    with open(p, \"br\") as f:\n",
    "        j =  json.load(f)\n",
    "    for line in j[\"sentences\"]:\n",
    "        chabsa_texts += [line[\"sentence\"].replace('\\n', '')]\n",
    "\n",
    "print(len(chabsa_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31b963-0999-4999-978c-7250f3f2e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 短すぎる文章と長すぎる文章を除く\n",
    "chabsa_texts = filter_by_length(chabsa_texts)\n",
    "print(len(chabsa_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521175d2-c651-4164-8c84-b6fd48889379",
   "metadata": {},
   "source": [
    "## データセット作成\n",
    "データセットはモデル開発用(dev)とテスト用(test)の2つ用意します。漱石の文章の8割を開発用に、残り2割をテスト用に使います。開発データには漱石の文章の1%の量だけ有価証券報告書の文章を混ぜます。テスト用に関しては、精度評価をしやすいように漱石の文章と有価証券報告書の文章は同数とします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0652bd3a-a31f-4325-b664-4d03349d7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 漱石にラベル0、有価証券報告書にラベル1を付与\n",
    "souseki_length = len(souseki_texts)\n",
    "souseki_arr = np.hstack([np.reshape(np.zeros(souseki_length), (-1, 1)), np.reshape(souseki_texts, (-1, 1))])\n",
    "chabsa_length = len(chabsa_texts)\n",
    "chabsa_arr = np.hstack([np.reshape(np.ones(chabsa_length), (-1, 1)), np.reshape(chabsa_texts, (-1, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab30de-5746-450b-aa34-296e7e1b0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データの数を決定\n",
    "num_s_dev = int(souseki_length * 0.8)\n",
    "num_c_dev = int(num_s_dev * 0.01)\n",
    "num_s_test = souseki_length - num_s_dev\n",
    "num_c_test = num_s_test\n",
    "print(\"開発データ　 漱石:{}, 有報:{}\".format(num_s_dev, num_c_dev))\n",
    "print(\"テストデータ 漱石:{}, 有報:{}\".format(num_s_test, num_c_test))\n",
    "\n",
    "# data split\n",
    "s_dev, s_test = train_test_split(souseki_arr, train_size=num_s_dev)\n",
    "c_dev, c_test = train_test_split(chabsa_arr, train_size=num_c_dev)\n",
    "c_test, _ = train_test_split(c_test, train_size=num_c_test)\n",
    "\n",
    "# 漱石と有価証券報告書をシャッフル\n",
    "dev_arr = np.vstack([s_dev, c_dev])\n",
    "np.random.shuffle(dev_arr)\n",
    "test_arr = np.vstack([s_test, c_test])\n",
    "np.random.shuffle(test_arr)\n",
    "\n",
    "print(dev_arr.shape, test_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab23a18-7d9f-4258-9d31-e286e29a58e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a1765-c8f3-498c-8122-6bb81bfe5d3c",
   "metadata": {},
   "source": [
    "## 異常検知モデル開発\n",
    "Universal Sentence Encoder (USE) Multilingual, CNN版を使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c283a1-0c1e-4aaa-91f3-34260298d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルをtfhubからインポート\n",
    "use_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'\n",
    "embed = hub.load(use_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86623ace-722a-4e8e-9f0e-684279da8404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, batch_size=100):\n",
    "    \"\"\"\n",
    "    文章のリストを埋め込みベクトルのリスト(np.ndarray)に変換\n",
    "    \"\"\"\n",
    "    length = len(texts)\n",
    "    n_loop = int(length / batch_size) + 1\n",
    "    embeddings = embed(texts[: batch_size])\n",
    "    for i in range(1, n_loop):\n",
    "        arr = embed(texts[batch_size*i: min(batch_size*(i+1), length)])\n",
    "        embeddings = tf.concat([embeddings, arr], axis=0)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946c7ea-4b50-4eed-9b8b-b5793c92f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_embeddings = get_embeddings(dev_arr[:, 1])\n",
    "test_embeddings = get_embeddings(test_arr[:, 1])\n",
    "print(dev_embeddings.shape, test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b78da-6437-4efb-92dd-d471e197153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均方向の推定値$\\hat{\\mu}$を求める\n",
    "mu = np.mean(dev_embeddings, axis=0)\n",
    "mu /= np.linalg.norm(mu)\n",
    "print(mu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468feb1d-8525-4a5e-94a2-5f368ad4c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異常度を計算し、異常度の従うカイ2乗分布のパラメータ$\\hat{m}$と$\\hat{s}$を推定\n",
    "anom = 1 - np.inner(mu, dev_embeddings)\n",
    "anom_mean = np.mean(anom)\n",
    "anom_mse = np.mean(anom**2) - anom_mean**2\n",
    "mhat = 2 * anom_mean**2 / anom_mse\n",
    "shat = 0.5 * anom_mse / anom_mean\n",
    "print(\"mhat: {:.1f}\".format(mhat))\n",
    "print(\"shat: {:.3e}\".format(shat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c6492-25a5-4bb9-aa48-e7885d013b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# カイ2乗分布の確率密度関数PDFと累積分布関数CDFをプロット\n",
    "x = np.linspace(0, 1, 100)\n",
    "plt.plot(x, chi2.pdf(x, mhat, loc=0, scale=shat), c='r')\n",
    "plt.title(\"PDF\", fontsize=18)\n",
    "plt.xlabel(\"a\", fontsize=18)\n",
    "plt.hist(anom, bins=100, density=True, range=(0,1), color=(0,1,0,0.5))\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.xlim(0,1)\n",
    "plt.grid()\n",
    "plt.savefig(\"./PDF.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "plt.plot(x, chi2.cdf(x, mhat, loc=0, scale=shat), c='b')\n",
    "plt.title(\"CDF\", fontsize=18)\n",
    "plt.xlabel(\"a\", fontsize=18)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.xlim(0,1)\n",
    "plt.grid()\n",
    "plt.savefig(\"./CDF.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e126d3c-a843-43c6-9a97-f93ab7653da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_solver(alpha, mhat, shat, x_ini=0.8, eps=1.e-12, n_ite=100):\n",
    "    \"\"\"\n",
    "    Solve equation\n",
    "    $$1-\\alpha = \\int_0^x \\\\! dx\\, \\chi^2 (x|\\hat{m}, \\hat{s}) $$\n",
    "    for x by Newtonian method.\n",
    "    \"\"\"\n",
    "    x = x_ini\n",
    "    for i in range(n_ite):\n",
    "        xnew = x - (chi2.cdf(x, mhat, loc=0, scale=shat) - (1 - alpha)) / chi2.pdf(x, mhat, loc=0, scale=shat)\n",
    "        if abs(xnew - x) < eps:\n",
    "            print(\"iteration: \", i+1)\n",
    "            break\n",
    "        x = xnew\n",
    "    return xnew\n",
    "\n",
    "alpha = 0.01\n",
    "ath = iteration_solver(alpha, mhat, shat)\n",
    "print(\"ath: {:.4f}\".format(ath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aeeca4-93f9-467f-86ff-4cdd99d9c585",
   "metadata": {},
   "source": [
    "## 精度評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6975c-1950-4829-b5ae-53bff69a4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの異常度\n",
    "anom_test = 1 - np.inner(mu, test_embeddings)\n",
    "\n",
    "# 閾値より異常度が大きければ異常標本と判定\n",
    "predict = (anom_test > ath).astype(np.int)\n",
    "\n",
    "# 正解データ\n",
    "answer = test_arr[:, 0].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba042d1-76aa-4a03-b7b4-229c9ac20450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各種精度指標計算\n",
    "acc = accuracy_score(answer, predict)\n",
    "precision = precision_score(answer, predict)\n",
    "recall = recall_score(answer, predict)\n",
    "f1 = f1_score(answer, predict)\n",
    "cm = confusion_matrix(answer, predict)\n",
    "print(\"Accuracy: {:.3f}, Precision: {:.3f}, Recall: {:.3f}, F1: {:.3f}\".format(acc, precision, recall, f1))\n",
    "\n",
    "# 混同行列\n",
    "plt.rcParams[\"font.size\"] = 18\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=[0,1])\n",
    "cmd.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030cc3c4-deeb-4663-bcdf-f3bf9a6be75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータに対する異常値のヒストグラムを正常データと異常データに分けてプロット\n",
    "souseki_anom_test = anom_test[test_arr[:, 0]=='0.0']\n",
    "chabsa_anom_test = anom_test[test_arr[:, 0]=='1.0']\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "plt.plot(x, chi2.pdf(x, mhat, loc=0, scale=shat), c='k')\n",
    "plt.xlabel(\"a\", fontsize=18)\n",
    "plt.hist(souseki_anom_test, bins=100, density=True, range=(0,1.1), color=(0,1,0,0.8))\n",
    "plt.hist(chabsa_anom_test, bins=100, density=True, range=(0,1.1), color=(0,0,1,0.5))\n",
    "plt.plot([ath, ath], [0, 9], c='r')\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.xlim(0.3,1.1)\n",
    "plt.ylim(0,9)\n",
    "plt.grid()\n",
    "plt.savefig(\"./PDF_test.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a890b-66aa-456f-84fd-a4a7f157d6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}